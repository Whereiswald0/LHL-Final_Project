{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7acb03-3938-46f6-b060-e2de0cfb87a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Libraries for getting the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from docx import Document\n",
    "from src.modules import * #contains functions used in common with processing election and IRS data\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e80bf7a0-4448-418f-b282-0c788081f10c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filepath to our formatted datasets (see 'merge_State_IRS_data.py' for how these were generated)\n",
    "datasets_filepath = 'data/house_fec_irs_joined/' \n",
    "\n",
    "datasets= [file for file in os.listdir(datasets_filepath) if os.path.isfile(os.path.join(datasets_filepath, file))]\n",
    "\n",
    "### NOTE: Due to the nameing conventions used in merge_State_IRS_data.py, every even-indexed dataframe ([0),[2],[4],[6]..(etc. etc.) will be the election data merged with the IRS data from that year\n",
    "### Every odd-indexed dataframe will contain the election data for that year merged with the IRS data from that year LESS the values from the previous year.\n",
    "\n",
    "# Name an empty dataframe for the .csvs containing only IRS data from the year of the election\n",
    "house_IRS_f = pd.DataFrame()\n",
    "\n",
    "# Iterate through the list of csv files, merging the appropriate ones\n",
    "for i in range(0, len(datasets), 2): #selects ever-other dataframe, starting with the first.\n",
    "    csv_file = datasets[i]\n",
    "    data = pd.read_csv(os.path.join(datasets_filepath, csv_file))\n",
    "    house_IRS_f = pd.concat([house_IRS_f, data], axis=0, ignore_index=True)\n",
    "    \n",
    "# Drop any null columns (The number of columns in the IRS data increases year-to-year, bureaucratically)\n",
    "house_IRS_f = house_IRS_f.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2d950b-0665-41a0-8704-d07cf09a5394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Name an empty dataframe for .csvs containing IRS data LESS the previous year's data\n",
    "house_IRS_d = pd.DataFrame()\n",
    "\n",
    "# Iterate through the list of csv files, merging the appropriate ones\n",
    "for i in range(1, len(datasets), 2): #selects ever-other dataframe, starting with the first.\n",
    "    csv_file = datasets[i]\n",
    "    data = pd.read_csv(os.path.join(datasets_filepath, csv_file))\n",
    "    house_IRS_d = pd.concat([house_IRS_d, data], axis=0, ignore_index=True)\n",
    "    \n",
    "# Drop any null columns (The number of columns in the IRS data increases year-to-year, bureaucratically)\n",
    "house_IRS_d = house_IRS_d.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7567ba93-b02b-4af9-8e21-e7bee99c22ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compose the columns\n",
    "# 'Dem_Voteshare' is the percent of total votes received by candidates from the Democratic party\n",
    "# 'Dem_tot' is the total votes received by Dem. incumbants and challengers\n",
    "# 'Rep_tot' is the total votes received by Rep. incumbants and challengers\n",
    "house_IRS_f['Dem_Voteshare'] = (house_IRS_f['D0'] + house_IRS_f['D1'])/ (house_IRS_f['D0'] + house_IRS_f['D1'] + house_IRS_f['R0'] + house_IRS_f['R1'] + house_IRS_f['OTHER0'])\n",
    "house_IRS_f['Dem_tot'] = (house_IRS_f['D0'] + house_IRS_f['D1'])\n",
    "house_IRS_f['Rep_tot'] = (house_IRS_f['R0'] + house_IRS_f['R1'])\n",
    "# col_to_drop = ['D0', 'D1', 'OTHER0', 'R0', 'R1']\n",
    "\n",
    "# Compose the columns\n",
    "house_IRS_d['Dem_Voteshare'] = (house_IRS_d['D0'] + house_IRS_d['D1'])/ (house_IRS_d['D0'] + house_IRS_d['D1'] + house_IRS_d['R0'] + house_IRS_d['R1'] + house_IRS_d['OTHER0'])\n",
    "house_IRS_d['Dem_tot'] = (house_IRS_d['D0'] + house_IRS_d['D1']) \n",
    "house_IRS_d['Rep_tot'] = (house_IRS_d['R0'] + house_IRS_d['R1'])\n",
    "# col_to_drop = ['D0', 'D1', 'OTHER0', 'R0', 'R1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f98e71-5813-4bad-9bfd-f38f26ae1363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "house_IRS_f2 = house_IRS_f.drop(['STATE','County','D0', 'D1', 'OTHER0', 'R0', 'R1'],axis=1)\n",
    "\n",
    "house_IRS_d2 = house_IRS_d.drop(['STATE','County','D0', 'D1', 'OTHER0', 'R0', 'R1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e17d90-a766-4c09-bbaf-52bf9e0a8559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Libraries for visualizing the dfs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87724aa8-5a42-4d98-9590-203b1154441f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "# Opting for MinMax here, but run with StandardScaler() as well, see if it produces different results\n",
    "scaler_MM = MinMaxScaler()\n",
    "f2_data_scaled_MM = pd.DataFrame(scaler_MM.fit_transform(house_IRS_f2), columns=house_IRS_f2.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c01bc-7175-4d98-a07a-188b67750afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Detect outliers using Isolation Forest \n",
    "# ### This was recommended, but has not been deployed yet as I do not fully understand it\n",
    "# isolation_forest = IsolationForest(contamination=0.05, random_state=12)\n",
    "# is_inlier = isolation_forest.fit_predict(f2_data_scaled_MM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd96c6-d2fc-42d8-8667-7aad9112b5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create histograms for each feature\n",
    "for c in house_IRS_f2.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(f2_data_scaled_MM[c], bins=30, kde=True)\n",
    "    plt.title(f'Histogram for {c}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d9caf-7e72-49b4-b8ea-f70b57dd669f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create box plots for each feature\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(data=f2_data_scaled_MM)\n",
    "plt.title('Box Plot for Each Variable')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e4da415-5f64-44bf-810a-5eaeb5e9b979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "# Opting for MinMax here, but run with StandardScaler() as well, see if it produces different results\n",
    "scaler_ss = StandardScaler()\n",
    "f2_data_scaled_S = pd.DataFrame(scaler_ss.fit_transform(house_IRS_f2), columns=house_IRS_f2.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9341f-6f45-4a75-a0b6-3309ebc33d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create histograms for each feature\n",
    "for c in house_IRS_f2.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(f2_data_scaled_S[c], bins=30, kde=True)\n",
    "    plt.title(f'Histogram for {c}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ac037-ff04-41f1-b86d-949b986d2de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create box plots for each feature\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(data=f2_data_scaled_S)\n",
    "plt.title('Box Plot for Each Variable')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97d281-c7dd-473e-b743-a474dd030173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Sooooo that looks like a lot of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d631073c-afd3-4493-af5e-e4d7dc10db48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Let's at least try a basic Gridsearch and random forest modeling\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA # There are a LOT of features, so using PCA to reduce them seems like a good idea\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error #using MSE at first, remember to try other error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa4f7cd6-f117-496b-8f12-96a81e8fcb19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     36\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assign X and y\n",
    "X = house_IRS_f2.drop(['Dem_Voteshare', 'Dem_tot', 'Rep_tot'],axis=1)\n",
    "### when re-running, try with a y of ONLY \"Dem_Voteshare\"\n",
    "y = house_IRS_f2[['Dem_Voteshare','Dem_tot','Rep_tot']]\n",
    "\n",
    "# Scale data\n",
    "X_scaled = scaler_ss.fit_transform(X)\n",
    "\n",
    "\n",
    "# Perform PCA to reduce features\n",
    "num_components = 20  # Consider adjusting this value if results are unsatisfactory\n",
    "pca = PCA(n_components=num_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "# Split into train and test sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Assign model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# first pass at guessing hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# build grid search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9628d77-7e8a-4654-8b68-c1fbfba68c44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Step 1: Scale the data with StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Step 2: Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Step 3: Choose a model (Random Forest Regressor in this example)\n",
    "# model = RandomForestRegressor()\n",
    "\n",
    "# # Step 4: Define hyperparameter grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [None, 10, 20],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# # Step 5: Create GridSearchCV object\n",
    "# grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# # Step 6: Fit the model with GridSearchCV\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Step 7: Get the best parameters\n",
    "# best_params = grid_search.best_params_\n",
    "# print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# # Step 8: Make predictions on the test set using the best model\n",
    "# best_model = grid_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# # Step 9: Evaluate the model\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(f\"Mean Squared Error on Test Set: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c55e7193-7da2-4440-9fc9-16453a0eac9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Mean Squared Error on Test Set: 3847191539.314436\n"
     ]
    }
   ],
   "source": [
    "# Assign X and y\n",
    "X = house_IRS_f2.drop(['Dem_Voteshare', 'Dem_tot', 'Rep_tot'],axis=1)\n",
    "### when re-running in the future, try with a y of ONLY \"Dem_Voteshare\"\n",
    "y = house_IRS_f2[['Dem_Voteshare','Dem_tot','Rep_tot']]\n",
    "\n",
    "# Scale data with Min-Max and re-run\n",
    "X_scaled_MM = scaler_MM.fit_transform(X)\n",
    "\n",
    "# Perform PCA to reduce features\n",
    "num_components = 20  # Consider adjusting this value if results are unsatisfactory\n",
    "pca = PCA(n_components=num_components)\n",
    "X_pca = pca.fit_transform(X_scaled_MM)\n",
    "\n",
    "# Split into train and test sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Assign model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# first pass at guessing hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# build grid search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd2c6f1-fc04-4dee-b7dc-67244c4e4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Min-Max scalinging seems to produce highter MSEs...\n",
    "### Let's build a more robust gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b955ec02-c54f-4a52-bcdf-86d366404c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85315557-c9df-4078-b7dc-5c5fda3c9021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
